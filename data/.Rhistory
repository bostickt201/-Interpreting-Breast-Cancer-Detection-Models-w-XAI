l_model <- train(y~x,
data,
method = 'lm',
preProcess = c("center", "scale"),
trControl = trCtrl)
# train quadratic model
poly_model <- train(y~.,
data,
method = 'lm',
preProcess = c("center", "scale"),
trControl = trCtrl)
print(l_model)
print(poly_model)
set.seed(440)
trCtrl <- trainControl(method = "cv", number = 3)
# train linear model
l_model <- train(y~x,
data,
method = 'lm',
preProcess = c("center", "scale"),
trControl = trCtrl)
# train quadratic model
poly_model <- train(y~.,
data,
method = 'lm',
preProcess = c("center", "scale"),
trControl = trCtrl)
print(l_model)
print(poly_model)
data <- data.frame(x = score, y = gpa)
set.seed(440)
summary(lmp(y~x,data, perm="Exact"))
data <- cbind(x, y)
data
data <- cbind(x, y)
data[,1]
data <- cbind(x, y)
data[,1] %*% data[,2]
data <- cbind(x, y)
t(data[,1]) %*% data[,2]
data <- cbind(x, y)
t(data[,1]) %*% t(data[,2])
set.seed(440)
# set up data
data <- cbind(x, y)
n <- 15
mu <- 0
sig <- 0.159
new_y <- function(data, b_hat){
x <- t(t(data[,2]))
data <-rbind(data, x*b_hat[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
return(data)
}
new_b <- function(data) {
x <- data[,1:2]
y <- t(t(data[,3]))
QR <- qr(x) # get QR
return(solve.qr(QR, y)) # estimate b hat
}
out <- boot(data, statistic = new_b, R = 1000, ran.gen = new_y, sim = "parametric", mle = B_hat)
data <- cbind(x, y)
b <- c(0.379370,.00452)
x <- t(t(data[,2]))
rbind(data, x*b_hat[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
data <- cbind(x, y)
b <- matrix(c(0.379370,.00452), nrow = 2)
x <- t(t(data[,2]))
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
b
c(b, x)
b
x
b <- matrix(c(0.379370,.00452), nrow = 2)
x <- data[,2]
x*b
b <- matrix(c(0.379370,.00452), nrow = 2)
x <- data[,2]
b*x
matrix(c(0.379370,.00452), nrow = 2)
data[,2]
b*x
matrix(c(0.379370,.00452), nrow = 2)
data[,2]
b[2,1] *x
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
matrix(c(0.379370,.00452), nrow = 2)
data[,2]
b[2,1] * x + b[1,1] + matrix(rnorm(n, mean=mu, sd=sig)), nrow = 15)
matrix(c(0.379370,.00452), nrow = 2)
data[,2]
b[2,1] * x + b[1,1]
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
matrix(c(0.379370,.00452), nrow = 2)
data[,2]
b[2,1] * x + b[1,1] + rnorm(n, mean=mu, sd=sig)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
matrix(c(0.379370,.00452), nrow = 2)
data[,2]
b[2,1] * x + b[1,1] + matrix(rnorm(n, mean=mu, sd=sig), ncol = 15)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
b[2,1] * x + b[1,1] + matrix(rnorm(n, mean=mu, sd=sig), ncol = 15)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
b[2,1] * x + b[1,1] + matrix(rnorm(n, mean=mu, sd=sig), nrow = 15)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
b[2,1] * x + b[1,] + matrix(rnorm(n, mean=mu, sd=sig), ncol = 15)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
b[2,] * x + b[1,] + matrix(rnorm(n, mean=mu, sd=sig), ncol = 15)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
b[2,] * x + b[1,] + matrix(rnorm(n, mean=mu, sd=sig), ncol = 15)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
data <- cbind(x, y)
rbind(data, b[2,] * x + b[1,] + matrix(rnorm(n, mean=mu, sd=sig), ncol = 15))
data <- cbind(x, y)
b[2,] * x + b[1,] + matrix(rnorm(n, mean=mu, sd=sig), ncol = 15)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
data <- cbind(x, y)
data
#b[2,] * x + b[1,] + matrix(rnorm(n, mean=mu, sd=sig), ncol = 15)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
# libraries
Sys.setenv(RGL_USE_NULL=TRUE)
library(matlib)
library(stats)
library(MASS)
library(lmPerm)
library(boot)
library(caret)
# set up data
score <- c(576, 635, 558, 578, 666, 580, 555, 661, 651, 605, 653, 575, 545, 572, 594)
gpa <- c(3.39, 3.3, 2.81, 3.03, 3.44, 3.07, 3, 3.43, 3.36, 3.13, 3.12, 2.74, 2.76, 2.88, 2.96)
y <- matrix(gpa, nrow = 15) # set y
b0 <- matrix(1, nrow = 15) # set intercept
x <- matrix(score, nrow = 15) # set x
x <- cbind(b0,x) # combine intercept and x
B_hat <- solve(t(x) %*% x) %*% t(x) %*% y
B_hat
model <- lm(gpa ~ score)
summary(model)
QR <- qr(x) # get QR
solve.qr(QR, y) # estimate b hat
xtx = t(x) %*% x
xtx_s = svd(xtx) # get svd
xtx_inv = xtx_s$v %*% diag(1 / xtx_s$d) %*% t(xtx_s$u) # get inverse
xtx_inv %*% t(x) %*% y # estimate B
b_hat <- rep(0, 2) # set up vector for B hat
# define cost function
cost <- function(X, Y, par){
n <- length(Y)
return(sum((X %*% par - Y)**2)/(2*n))
}
optim(par = b_hat, fn = cost, X = x, Y = y, method = 'BFGS') # compute estimate
data <- cbind(x, y)
data
#b[2,] * x + b[1,] + matrix(rnorm(n, mean=mu, sd=sig), ncol = 15)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
data <- rbind(x, y)
data <- cbind(x, y)
data
#b[2,] * x + b[1,] + matrix(rnorm(n, mean=mu, sd=sig), ncol = 15)
#rbind(data, x*b[2] + b_hat[1]+  matrix(rnorm(n, mean=mu, sd=sig)), ncol = 15)
data <- cbind(x, y)
data
b[2,] * x + b[1,] + matrix(rnorm(n, mean=mu, sd=sig), nrow = 15)
b[2,] * x + b[1,] + matrix(rnorm(n, mean=mu, sd=sig), nrow = 15)
data <- cbind(x,y)
data
data <- cbind(x,y)
X <- data[,2]
Y <- data[,3]
X
data <- cbind(x,y)
X <- data[,2]
Y <- data[,3]
Y
data <- cbind(x,y)
B0 <- data[,1]
X <- data[,2]
Y <- data[,3]
Y
data <- cbind(x,y)
B0 <- data[,1]
X <- data[,2]
Y <- data[,3]
B <- c(0.37937,0.00452)
B[2,] * X
B[2,] * t(X)
B[2,] * t(X)
t(X)
t(t(X))
B[2,] * t(t(X))
B[2,1] * t(t(X))
B[2,1]
B[,1]
B
data <- cbind(x,y)
B0 <- data[,1]
X <- data[,2]
Y <- data[,3]
B <- matrix(c(0.37937,0.00452), nrow = 2)
B
data <- cbind(x,y)
B0 <- data[,1]
X <- data[,2]
Y <- data[,3]
B <- matrix(c(0.37937,0.00452), nrow = 2)
X
B
B[2,] * Y
B[2,] * Y + B[1,]
B[2,] * Y + B[1,] + rnorm(n, 0, 15)
B[2,] * Y + B[1,] + rnorm(n, 0, 0.159)
B[2,] * Y + B[1,] + rnorm(n, 0, 0.159)
data
B[2,] * Y + B[1,] + rnorm(n, 0, 0.159)
library(boot)
library(distr6)
set.seed(440)
# current data
n <- 12
Sn <- 4
X <- rbinom(n, 1, Sn/n)
theta_hat = function(data, size){ # get theta estimate
return(sum(data)/size)
}
# perform bootstrap
sample_boot <- replicate(n=1000,{
# Sample with replacement from original data sample
size <- 12
boot_data <- sample(X, size, replace=TRUE)
# Compute theta hat
theta_hat(boot_data, size)
})
# get CI
c(lower = mean(sample_boot) - 2 * sd(sample_boot),
est = mean(sample_boot),
upper = mean(sample_boot) + 2 * sd(sample_boot))
set.seed(440)
Sn <- 4
n <- 12
m <- 1000
Y <- rbeta(m, 2, 2) # proposal samples
w <- dbinom(Sn, size = n, prob = Y, log = TRUE)
sum(Y * w) / sum(w)
set.seed(440)
conf <- 0.05
a <- 5
b <- 9
lower <- qbeta(conf/2, a, b)
upper <- qbeta(1 - conf/2, a, b)
c(lower, upper)
set.seed(440)
theta_0 <- 0 # initial theta
f1 = function(theta){
return(4/theta - 8/(1-theta))
}
f2 = function(theta){
return(-4/(theta^2) - 8/((1-theta)^2))
}
newtons_method_1d = function(start, d1, d2, tol) {
prev_step = start
new_step = start
num_iters = 0
while (TRUE) {
new_step = prev_step - d1(prev_step) / d2(prev_step)
num_iters = num_iters + 1
# check for convergence
if (abs(new_step - prev_step) <= tol) {
print("Converged.")
return(list(result=new_step,iters=num_iters)
)} else {print(paste0("Iter: ", num_iters, ", Value: ", new_step))
prev_step = new_step
}
}
}
newton_res = newtons_method_1d(0, f1, f2, tol=.001)
set.seed(440)
theta_0 <- 0 # initial theta
f1 = function(theta){
return(4/theta - 8/(1-theta))
}
f2 = function(theta){
return(-4/(theta^2) - 8/((1-theta)^2))
}
newtons_method_1d = function(start, d1, d2, tol) {
prev_step = start
new_step = start
num_iters = 0
while (TRUE) {
new_step = prev_step - d1(prev_step) / d2(prev_step)
num_iters = num_iters + 1
# check for convergence
if (abs(new_step - prev_step) <= tol) {
print("Converged.")
return(list(result=new_step,iters=num_iters)
)} else {print(paste0("Iter: ", num_iters, ", Value: ", new_step))
prev_step = new_step
}
}
}
newton_res = newtons_method_1d(0.25, f1, f2, tol=.001)
set.seed(440)
theta_0 <- 0 # initial theta
f1 = function(theta){
return(4/theta - 8/(1-theta))
}
f2 = function(theta){
return(-4/(theta^2) - 8/((1-theta)^2))
}
newtons_method_1d = function(start, d1, d2, tol) {
prev_step = start
new_step = start
num_iters = 0
while (TRUE) {
new_step = prev_step - d1(prev_step) / d2(prev_step)
num_iters = num_iters + 1
# check for convergence
if (abs(new_step - prev_step) <= tol) {
print("Converged.")
return(list(result=new_step,iters=num_iters)
)} else {print(paste0("Iter: ", num_iters, ", Value: ", new_step))
prev_step = new_step
}
}
}
newton_res = newtons_method_1d(0.333, f1, f2, tol=.001)
set.seed(440)
theta_0 <- 0 # initial theta
f1 = function(theta){
return(4/theta - 8/(1-theta))
}
f2 = function(theta){
return(-4/(theta^2) - 8/((1-theta)^2))
}
newtons_method_1d = function(start, d1, d2, tol) {
prev_step = start
new_step = start
num_iters = 0
while (TRUE) {
new_step = prev_step - d1(prev_step) / d2(prev_step)
num_iters = num_iters + 1
# check for convergence
if (abs(new_step - prev_step) <= tol) {
print("Converged.")
return(list(result=new_step,iters=num_iters)
)} else {print(paste0("Iter: ", num_iters, ", Value: ", new_step))
prev_step = new_step
}
}
}
newton_res = newtons_method_1d(0.2, f1, f2, tol=.001)
set.seed(440)
theta_0 <- 0.25 # initial theta
f1 = function(theta){
return(4/theta - 8/(1-theta))
}
f2 = function(theta){
return(-4/(theta^2) - 8/((1-theta)^2))
}
newtons_method_1d = function(start, d1, d2, tol) {
prev_step = start
new_step = start
num_iters = 0
while (TRUE) {
new_step = prev_step - d1(prev_step) / d2(prev_step)
num_iters = num_iters + 1
# check for convergence
if (abs(new_step - prev_step) <= tol) {
print("Converged.")
return(list(result=new_step,iters=num_iters)
)} else {print(paste0("Iter: ", num_iters, ", Value: ", new_step))
prev_step = new_step
}
}
}
newton_res = newtons_method_1d(theta_0), f1, f2, tol=.001)
set.seed(440)
theta_0 <- 0.25 # initial theta
f1 = function(theta){
return(4/theta - 8/(1-theta))
}
f2 = function(theta){
return(-4/(theta^2) - 8/((1-theta)^2))
}
newtons_method_1d = function(start, d1, d2, tol) {
prev_step = start
new_step = start
num_iters = 0
while (TRUE) {
new_step = prev_step - d1(prev_step) / d2(prev_step)
num_iters = num_iters + 1
# check for convergence
if (abs(new_step - prev_step) <= tol) {
print("Converged.")
return(list(result=new_step,iters=num_iters)
)} else {print(paste0("Iter: ", num_iters, ", Value: ", new_step))
prev_step = new_step
}
}
}
newton_res = newtons_method_1d(theta_0, f1, f2, tol=.001)
set.seed(440)
theta_0 <- 0.25 # initial theta
f1 = function(theta){
return(4/theta - 8/(1-theta))
}
f2 = function(theta){
return(-4/(theta^2) - 8/((1-theta)^2))
}
newtons_method_1d = function(start, d1, d2, tol) {
prev_step = start
new_step = start
num_iters = 0
while (TRUE) {
new_step = prev_step - d1(prev_step) / d2(prev_step)
num_iters = num_iters + 1
# check for convergence
if (abs(new_step - prev_step) <= tol) {
print("Converged.")
return(list(result=new_step,iters=num_iters)
)} else {print(paste0("Iter: ", num_iters, ", Value: ", new_step))
prev_step = new_step
}
}
}
newton_res = newtons_method_1d(theta_0, f1, f2, tol=.001)
library(boot)
library(distr6)
set.seed(440)
# current data
n <- 12
Sn <- 4
X <- rbinom(n, 1, Sn/n)
theta_hat = function(data, size){ # get theta estimate
return(sum(data)/size)
}
# perform bootstrap
sample_boot <- replicate(n=1000,{
# Sample with replacement from original data sample
size <- 12
boot_data <- sample(X, size, replace=TRUE)
# Compute theta hat
theta_hat(boot_data, size)
})
# get CI
c(lower = mean(sample_boot) - 2 * sd(sample_boot),
est = mean(sample_boot),
upper = mean(sample_boot) + 2 * sd(sample_boot))
set.seed(440)
Sn <- 4
n <- 12
m <- 1000
Y <- rbeta(m, 2, 2) # proposal samples
w <- dbinom(Sn, size = n, prob = Y, log = TRUE)
sum(Y * w) / sum(w)
set.seed(440)
conf <- 0.05
a <- 5
b <- 9
lower <- qbeta(conf/2, a, b)
upper <- qbeta(1 - conf/2, a, b)
c(lower, upper)
set.seed(440)
theta_0 <- 0.25 # initial theta
f1 = function(theta){
return(4/theta - 8/(1-theta))
}
f2 = function(theta){
return(-4/(theta^2) - 8/((1-theta)^2))
}
newtons_method_1d = function(start, d1, d2, tol) {
prev_step = start
new_step = start
num_iters = 0
while (TRUE) {
new_step = prev_step - d1(prev_step) / d2(prev_step)
num_iters = num_iters + 1
# check for convergence
if (abs(new_step - prev_step) <= tol) {
print("Converged.")
return(list(result=new_step,iters=num_iters)
)} else {print(paste0("Iter: ", num_iters, ", Value: ", new_step))
prev_step = new_step
}
}
}
newton_res = newtons_method_1d(theta_0, f1, f2, tol=.001)
setwd("~/Downloads/archive
")
setwd("./Downloads/archive
")
setwd("./Downloads/archive")
MNIST <- read.csv(mnist_test.csv, header = TRUE)
MNIST <- read.csv("mnist_test.csv", header = TRUE)
View(MNIST)
MNIST <- MNIST[1:1000,]
View(MNIST)
write.csv(MNIST, "/MNIST.csv", row.names = FALSE)
write.csv(MNIST, "MNIST.csv", row.names = FALSE)
setwd("~/data/wisconsin")
setwd("./data/wisconsin")
getwd()
setwd("~/Desktop/DS340W/term_project/data/wisconsin")
wisc <- read.csv("data.csv", header = TRUE)
View(wisc)
wisc <- wisc[,c(1:32)]
write.csv(wisc, "data.csv", row.names = FALSE)
wisc <- read.csv("data.csv", header = TRUE)
View(wisc)
wisc$diagnosis<- as.fact(wisc$diagnosis)
wisc$diagnosis<- as.factor(wisc$diagnosis)
levels(wisc$diagnosis)
table(wisc$diagnosis)
